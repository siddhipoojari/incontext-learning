# In-Context Learning: Procedural and Creative Prompting for ChatGPT

This repository documents a project that explores the in-context learning capabilities of large language models (LLMs), specifically focusing on procedural and creative writing tasks using ChatGPT, Claude.ai, and Google Bard. The project evaluates how well these models can generate coherent and relevant content based on structured prompts.

## ðŸ“š Project Overview

### Abstract
In this project, we test the performance of LLMs on procedural and creative writing tasks, assessing their outputs through both human evaluators and the models themselves. We examine challenges in prompt engineering, adherence to constraints, and the models' tendencies to overestimate the quality of their outputs.

### Key Components

- **Procedural Writing Tasks:** Evaluating the ability of LLMs to generate instructional content effectively.
- **Creative Writing Tasks:** Replicating aspects of the Tree of Thoughts and Chain of Thoughts experiment, focusing on reasoning abilities and structured planning in response to prompts.

### Methodology

- Consistent prompts were crafted for trials, with a focus on effective prompt engineering to ensure quality outputs.
- Evaluations were conducted by both the authors and the LLMs, providing a comprehensive assessment of the generated content.
- The public version of ChatGPT 3.5, along with Google Bard and Claude.ai, were utilized to generate and evaluate responses.

## ðŸš€ Technologies Used

- **Large Language Models:** ChatGPT, Claude.ai, Google Bard
- **Python:** For implementing the experimentation framework
- **Excel:** For analyzing the responses
  
## ðŸ“„ Getting Started

To explore the details of the project, including prompts, outputs, and evaluation criteria, please refer to the individual files and documentation within the repository.

## ðŸ“« Contact

For any questions or further inquiries, feel free to reach out to me at [siddhi.poojari@utexas.edu](mailto:siddhi.poojari@utexas.edu).
